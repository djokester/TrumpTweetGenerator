{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Generating the Model \n",
    "\n",
    "## Libraries used are \n",
    "* nltk\n",
    "* pandas\n",
    "* keras\n",
    "* numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Tweets \n",
    "We read the tweets off the data.csv file, where all the tweets are kept. \n",
    "Then we tokenize the tweets with NLTK's tweet tokenizer to strip off the handles and hashtags. \n",
    "Next we join the list of tokens to get back a string, which we convert into a list of characters. \n",
    "We run a list comprehension over the list of characters to remove all non-alphabet characters like numbers and punctuation marks. This is to reduce the character space and hence preserve computational power for faster and more effective training. \n",
    "Then we find the list of unique characters by converting the list of characters into a set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweets = data[\"Text\"].tolist()\n",
    "tweets = [str(i) for i in tweets]\n",
    "text = tknzr.tokenize(\" \".join(tweets))\n",
    "text = [i.lower() for i in text if i.isalpha() == True]\n",
    "tweet = \" \".join(text)\n",
    "data = list(tweet)\n",
    "chars = list(set(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Text into NumPy Array Sequences. \n",
    "The size of the character space is 36. \n",
    "First we build two functions. One to convert characters to index based on the index of the respective characters in the variable ***chars*** and another to find the character given the index. Next we start representing character sequences as numpy arrays of dimensions (Sequence Length * Character Space) with each character being denoted along axis=1 using 1, and the rest of the values being kept as zeros. The Y sequence is one character ahead of the X sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexOfChar = {ix:char for ix, char in enumerate(chars)}\n",
    "charOfIndex = {char:ix for ix, char in enumerate(chars)}\n",
    "\n",
    "length = 140\n",
    "charspace = len(chars)\n",
    "X = np.zeros((int(len(data)/length), length, charspace))\n",
    "y = np.zeros((int(len(data)/length), length, charspace))\n",
    "if 1 == 1:\n",
    "    for i in range(0, int(len(data)/length)):\n",
    "        X_sequence = data[i*length:(i+1)*length]\n",
    "        X_sequence_ix = [charOfIndex[value] for value in X_sequence]\n",
    "        input_sequence = np.zeros((length, charspace))\n",
    "        for j in range(length):\n",
    "            input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "            X[i] = input_sequence\n",
    "\n",
    "        y_sequence = data[i*length+1:(i+1)*length+1]\n",
    "        y_sequence_ix = [charOfIndex[value] for value in y_sequence]\n",
    "        target_sequence = np.zeros((length, charspace))\n",
    "        for j in range(length):\n",
    "            target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "            y[i] = target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "The model consists of four LSTM layers, with a dropout of 0.2 after the last three layers. We use the softmax activation function as this is a multiclass classification problem, where we try to find the character immediately after the given sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model = Sequential()\n",
    "model.add(LSTM(1200, input_shape=(None, charspace), return_sequences=True))\n",
    "for i in range(3):\n",
    "    model.add(LSTM(int(1000- i*100), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(charspace)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, length, charspace, indexOfChar):\n",
    "    # starting with random character\n",
    "    ix = [np.random.randint(charspace)]\n",
    "    y = [indexOfChar[ix[-1]]]\n",
    "    X = np.zeros((1, length, charspace))\n",
    "    for i in range(length):\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(indexOfChar[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y.append(indexOfChar[ix[-1]])\n",
    "    return ('').join(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for 200 epochs, and try generating sequences after every epoch to check progress. Save model when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 0\n",
    "while epochs< 200:\n",
    "    print('\\n\\nEpoch: {}\\n'.format(epochs))\n",
    "    model.fit(X, y, batch_size=64, verbose=1, epochs=1)\n",
    "    epochs += 1\n",
    "    generate_text(model, 140, charspace, indexOfChar)\n",
    "\n",
    "model.save(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
